# processing-household-composition-data

The scripts in this repository construct histograms of age-stratified household compositions from census data, primarily for use in infectious disease modelling. There are a few key pieces of terminology we will use in this readme. An *age class* is the set of people in a population who are in a specific age range. A *household* is a group of indivduals who are grouped together for epidemiological purposes, typically because they live together and/or form a nuclear family. A *household composition* is a vector indexed by the set of age classes which specifies the number of individuals in each age class present in a household. The composition of a household is defined up to the age classes chosen, so that the set of compositions present in a population are dependent on the age classes being used. The functions provided in this repository are designed to produce histograms of household compositions based on user-specified age classes, and to any of the spatial resolutions for which raw data exists. A major restriction is that we only allow for age classes which are a *coarsening* of the age classes used in the raw data, in the sense that each of the user-specified age classes must be either one of the age classes in the raw data or a union of consecutive age classes. For instance, if the raw data has age classes (0-20,20-40,40-60,60-80,80+), then (0-40.40-60.60+) is allowed but (0-35,35-45.45-60,60+) is not.

These scripts are primarily designed for processing the household data which is available from the UK's Office for National Statistics (ONS). The `preprocessing` subfolder is for scripts which reformat specific datasets to make them compatible with the rest of the code in the repository.

## functions

### `filter_rare_households_ONS.m`

This function is used to remove the largest households from a datase. It takes a household composition dataset in ONS format and a number *p* as arguments, and constructs a weighted histogram of household sizes, such that the size of the *n*th bin in the histogram is the proportion of the population belonging to households of size $n$ (**not** the proportion of households which are size *n*). Using this histogram, we calculate the proportion of the population in households of size *n* or higher, for *n* ranging from 1 to the maximum household size found in the composition data. From these proportions we choose a size threshold *N* such that *N* is the smallest possible household size for which the proportion of the population in households of size *N* or larger is less than *p*. All households of size *N* or larger are removed from the household composition data, leaving us with a version of the original composition data which has been filtered to minimise the maximum household size while keeping the proportion of the population removed from the data below *p*. For instance, if we set *p=0.05* and find that 7.5\% of the population belong to households of size 6 or larger and 2.5% belong to households of size 7 or larger, then `filter_rare_households_ONS` will remove all households of size 7 or larger, because removing the households of size 6 will mean we have removed more than 5% of the population, while on the other hand keeping the households of size 7 is not necessary in light of the proportion constraint. The motivation for removing large households is that the number of epidemiological states a household can occupy increases dramatically with the household size and the number of age classes present, meaning that including large, rare household compositions in a model population substantially increases system size and computational intensity in exchange for a relatively small payoff in terms of increased accuracy.

### `build_HH_dist_from_ONS_data.m`

This function constructs a distribution or set of distributions of household compositions at a specified spatial resolution based on a household composition dataset in ONS format. The ONS data specifies the compositions present in each of a given low-level output area (ENUMOA in the CT1088 data) and the frequencies at which they appear, and for each such datapoint it specifies the higher-level output areas to which the low-level output area belongs (the different level output areas are nested so that each lower level output area belongs to exactly one output area at the next lowest level). In practice, we would like to know the compositions present, and their frequencies, in higher-level output areas; for instance, we may wish to model infectious disease dynamics for the whole of England and Wales rather than any particular region in either country. This requires aggregating all of the household compositions and frequencies for all the lowest-level output areas contained in each of the output areas at the resolution we are interested in; to model the whole of England and Wales we would need to aggregate all of the data into a single non-repeating list of compositions, while to model each of England and Wales we would need to aggregate the compositions and frequencies from the lowest-level output areas in England into one histogram and those from Wales into another. This turns out to be an extremely computationally intensive task because to calculate the number of times a given frequency appears in a given higher-level output area we need to search for all of its appearances in the lowest-level output areas lying within that higher-level output area. This is a computationally intensive task because under a naive approach a comparison between two compositions will involve a comparison between the numbers of individuals in each age class in the two compositions. If the composition dataset is in terms of *K* age classes, then each comparison operation will involve up to *K* comparisons. Because of the large size of low-level output area household composition datasets (the CT1088 dataset contains over eight million datapoints) it is important to perform any operations on the data as efficiently as possible. To this end, `build_HH_dist_from_ONS_data` proceeds by encoding each composition as a unique integer, meaning each comparison is reduced to a single operation. With the compositions encoded as integers, we can replace the columns corresponding to the compositions in the original dataset with a single composition containing the appropriate integers. For each output area we then construct a histogram of the integers which appear in rows where the appropriate output area code appears, and obtain a distribution of household compositions for each output area by replacing the integers with the compositions they encode. Note that in the code as it appears here, we find the rows corresponding to a given output area using a string comparison, although it would be more efficient to also encode these strings as integers, and we will do this in a future update to the repository.

The encoding we use is as follows. Let *N_k* be the number of individuals of age class *k* present in a given household composition, and let $M_k$ be the maximum number of individuals of age class *k* which is observed in a single household across the entire population. We encode each household according to the formula <img src="https://render.githubusercontent.com/render/math?math=N_1%2B\sum_k N_k\prod_{j=1}^{k-1}(1%2BM_j)">. To see that this formula does indeed uniquely encode the compositions, compare with the representation of the integers in base ten, where, for instance, the number 1,234 is encoded as <img src="https://render.githubusercontent.com/render/math?math=1\times10^3%2B2\times10^2%2B3\times10^1%2B4\times10^0">. For the purposes of this analogy, it is important to emphasise that the powers of ten represent successive multiplications by the number of possible values the digit in the following places can take, i.e. a 3 in the tens column represents three successive cycles over all the values that can go in the units column. Our encoding of the household compositions does a very similar thing, but the maximum number of individuals of each age class replaces the factors of 10, because now the number of values to cycle over in each column is given by the maximum value observed in the population. While this is not the most efficient possible encoding (where a more efficient encoding means one that produces smaller integers, which is desirable because very large numbers can not be stored as 64-bit integers), it has the advantage of being easy to explain and implement in code.
